{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from datetime import datetime \n",
    "from functions.Team_Augury_load_transform_saved import load_and_preprocess\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cla, figure\n",
    "import altair as alt\n",
    "from altair_saver import save\n",
    "from sklearn.inspection import permutation_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features...\n",
      "data loaded and pre_processed\n",
      "Testing load, length of X, y: (1674, 776) (1674,)\n",
      "type <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_and_preprocess()\n",
    "print ('Testing load, length of X, y:', X_train.shape, y_train.shape)\n",
    "print ('type', type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model load: Accuracy result for GBT: 0.9097968936678614\n"
     ]
    }
   ],
   "source": [
    "#load pkl'd SVC clf\n",
    "filename = \"models/SVC_vanilla_model.sav\" #note this is the 'Vanillia model', not the optimised tuned one, assume kernel = rbf\n",
    "SVC_loaded = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#test load\n",
    "result = SVC_loaded.score(X_train, y_train)\n",
    "print('Testing model load: Accuracy result for GBT:' , result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get perm/feature importances ## NB THIS TOOK 88 mins, so written to pkl for future uses \n",
    "# perm_importance = permutation_importance(SVC_loaded, X_train, y_train)\n",
    "# print (len(perm_importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"models/SVM_feat_imp_bunch_object_vanilla_model.sav\"\n",
    "#pickle.dump(perm_importance, open(filename, 'wb')) #the code to dump it\n",
    "perm_importance = pickle.load(open(filename, 'rb')) #the code to load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(perm_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = perm_importance.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(list(X_train.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unhash this block to create viz of feature importance for all 776 features sorted by size\n",
    "# figure(figsize=(10, 80), dpi=100)\n",
    "# plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "# plt.xlabel(\"Permutation Importance\")\n",
    "# plt.savefig(\"saved_work/SVC_long_feat_imp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_idx = np.flatnonzero(np.char.find(features, \"bert\")!=-1)\n",
    "len(bert_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'post_sbert_384'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[388] #shows post_sbert are 5:388 and comments sbert are 392:775 in between are 389:392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37132616487456205 0.12162485065711344\n"
     ]
    }
   ],
   "source": [
    "sbert_post_sum = np.sum(perm_importance.importances_mean[5:388])\n",
    "sbert_comm_sum = np.sum(perm_importance.importances_mean[392:775])\n",
    "print (sbert_post_sum, sbert_comm_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['post_author_karma' 'number_comments_vs_hrs' 'post_sentiment' 'time_hour'\n",
      " 'day_of_week' 'avg_comment_upvotes_vs_hrs' 'avg_comment_author_karma'\n",
      " 'avg_comment_sentiment']\n",
      "[ 0.00657109  0.01015532 -0.00023895  0.00728793  0.00549582  0.01565114\n",
      "  0.00669056  0.00059737]\n"
     ]
    }
   ],
   "source": [
    "# Having located the sbert items they now need removing from both feature names and scoring arrays\n",
    "ind = np.indices(features.shape)[0]\n",
    "mylist=(slice(5,389),slice(392,776))\n",
    "rm = np.hstack([ind[i] for i in mylist])\n",
    "simplified_names = np.take(features, sorted(set(ind)-set(rm)))\n",
    "print(simplified_names) # should be small enough for visual confirmation\n",
    "simplified_fi_scores = np.take(perm_importance.importances_mean, sorted(set(ind)-set(rm)) )\n",
    "print (simplified_fi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now just need to add sbert agregated names and scores to each array\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
